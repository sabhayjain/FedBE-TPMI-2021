{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResnetFEDAVG_NonIID.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy46_-wlEr5g",
        "outputId": "01aed4fc-112a-456b-cb3a-24ea2017177b"
      },
      "source": [
        "!pip install keras-swa==0.1.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-swa==0.1.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/40/2796068a5d40c9b7f8d2b0d5a830a45c3ff9909194931f50a87ec19f45d5/keras-swa-0.1.5.tar.gz (77kB)\n",
            "\r\u001b[K     |████▎                           | 10kB 22.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20kB 29.4MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30kB 34.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40kB 38.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51kB 41.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 61kB 37.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 71kB 36.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 10.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: keras-swa\n",
            "  Building wheel for keras-swa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-swa: filename=keras_swa-0.1.5-cp37-none-any.whl size=9714 sha256=d4b60e0b854c16b6ce7874311bd41a749d5ff214e64e61b1b2b7c9e55c2103fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/de/a7/f2a71861bb678729a45e8995dc2496118e875e64b89379c530\n",
            "Successfully built keras-swa\n",
            "Installing collected packages: keras-swa\n",
            "Successfully installed keras-swa-0.1.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S16nLNsosyNk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f2a0974-5475-477d-8a4a-000fad8ac31c"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon May 17 16:38:05 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wOBNu8TEu1A"
      },
      "source": [
        "from swa.tfkeras import SWA\n",
        "import tensorflow.keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stJrFfne1AtY"
      },
      "source": [
        "from keras.datasets import cifar10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8KlZ2ox1GwS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7d8671a-32df-4e0d-e004-2917efff4924"
      },
      "source": [
        "(trainX, trainy), (testX, testy) = cifar10.load_data()\n",
        "trainX = trainX/255\n",
        "testX = testX/255"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hI7kn4F14Uv"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "from imutils import paths\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras import backend as K\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0XVVFJt2P-u"
      },
      "source": [
        "lb = LabelBinarizer()                                  #Binarize labels in a one-vs-all fashion.\n",
        "trainy = lb.fit_transform(trainy)                     #Fit label binarizer and transform multi-class labels to binary labels\n",
        "testy = lb.fit_transform(testy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "825_bYug4h01"
      },
      "source": [
        "def label_dict(img, labels):  # creating dictionary of labels\n",
        "  lab_lis = {}\n",
        "  for i in range(10):\n",
        "    lab_lis[i] = []          #declaring key values as labels  in dictionary\n",
        "  data = list(zip(img, labels))  # list of tuples of each image and label pair\n",
        "  for im, lab in data:\n",
        "    idx = np.argmax(lab)\n",
        "    lab_lis[idx].append((im, lab))\n",
        "  return lab_lis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_J40L6C4jdf"
      },
      "source": [
        "t = label_dict(trainX, trainy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axv59HWr5w0e"
      },
      "source": [
        "unlab_data = []\n",
        "unlab_img_data = []\n",
        "unlab_img_lab = []\n",
        "for i in range(len(t)):\n",
        "  unlab_data.extend(t[i][4000:]) # extracting 1000 imgaes from each class in the training set to generate the pool of so called unlabeled set\n",
        "for x,y in unlab_data:\n",
        "  unlab_img_data.append(x)\n",
        "  unlab_img_lab.append(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIbri0xe79a2"
      },
      "source": [
        "client_names = [\"client_\"+str(i) for i in range(10)]  # makiing non_iid datas set for each client\n",
        "client_data = {}\n",
        "for client in client_names:\n",
        "  client_data[client] = []\n",
        "i = 0\n",
        "for client in client_names:\n",
        "  client_data[client].extend(t[i%10][:1960])\n",
        "  client_data[client].extend(t[(i+1)%10][1960:3920])\n",
        "  i += 1\n",
        "for i in range(10):\n",
        "  k = 0\n",
        "  for j in range(10):\n",
        "    if j != (i)%10 and j != (i+1)%10:\n",
        "      client_data[\"client_\"+str(j)].extend(t[i][3920 + 10*k: 3920 + 10*(k+1)])\n",
        "      k += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQitzXbYCdZj"
      },
      "source": [
        "def batch(zip_data, batch_size = 40):\n",
        "  data, label = zip(*zip_data)  # Unzipping the img and label pair \n",
        "  dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label))) # create a dataset object to make use of other transformation attributes\n",
        "  return dataset.shuffle(len(label)).batch(batch_size)# shuffles data and creates batches of size 40"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwMs6hlaC7xH"
      },
      "source": [
        "client_batched = {}\n",
        "for (client_name, data) in client_data.items():\n",
        "  client_batched[client_name] = batch(data)\n",
        "\n",
        "test_batched = tf.data.Dataset.from_tensor_slices((testX, testy)).batch(len(testy))\n",
        "unlab_dataset = tf.data.Dataset.from_tensor_slices((unlab_img_data)).batch(len(unlab_img_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7wnWo0-BYQp",
        "outputId": "aa05b4a4-20b7-47aa-8ec8-62e31657d87b"
      },
      "source": [
        "\n",
        "print(list(client_batched.keys()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['client_0', 'client_1', 'client_2', 'client_3', 'client_4', 'client_5', 'client_6', 'client_7', 'client_8', 'client_9']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5Lz3GrdGk6H"
      },
      "source": [
        "class MLP:\n",
        "    @staticmethod\n",
        "    def build():\n",
        "        tf_input = Input(shape=(32, 32,  3))\n",
        "        base_model = ResNet50(input_tensor=tf_input, include_top=False)\n",
        "        x = base_model.output\n",
        "        x = GlobalAveragePooling2D()(x)\n",
        "        x = Dense(1024, activation='relu')(x)\n",
        "        predictions = Dense(10, activation='softmax')(x)\n",
        "        model = Model(inputs=base_model.input, outputs=predictions)\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2eVLxQaKGEV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6abf9366-50ae-4f72-8e14-b96a2b56287c"
      },
      "source": [
        "model = MLP()\n",
        "model = model.build()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BmrbqbTKO37",
        "outputId": "97c8c2dd-18ad-4267-88af-64deac584ebe"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 2048)         0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1024)         2098176     global_average_pooling2d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           10250       dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 25,696,138\n",
            "Trainable params: 25,643,018\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BzZ4rWaLpN4",
        "outputId": "87e63bdc-6ab6-4e93-9fca-3e3fd6bfc16f"
      },
      "source": [
        "t = model.get_weights()  # (3,3,3,32) => 3 RGB channels, and 32 filters of size (3,3)  => 3*3*3*32 + 32( bias) = 896 (no of param of 1st layer)\n",
        "                         # (32,) refers to the bias weights \n",
        "                         #(3,3,32,32) => 32 filters ouputs of first layer, 32 filters  of size (3,3)of current layer => 32*3*3*32 +32 = 9248\n",
        "for v in t:                \n",
        "  print(len(v))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "1\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "3\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "1\n",
            "256\n",
            "1\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "1\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "3\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "1\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "1\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "3\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "1\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "1\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "3\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "1\n",
            "512\n",
            "1\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "1\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "3\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "1\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "1\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "3\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "1\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "1\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "3\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "128\n",
            "1\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "1\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "3\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "1\n",
            "1024\n",
            "1\n",
            "1024\n",
            "1024\n",
            "1024\n",
            "1024\n",
            "1024\n",
            "1024\n",
            "1024\n",
            "1024\n",
            "1024\n",
            "1\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "3\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "1\n",
            "1024\n",
            "1024\n",
            "1024\n",
            "1024\n",
            "1024\n",
            "1\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "3\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "1\n",
            "1024\n",
            "1024\n",
            "1024\n",
            "1024\n",
            "1024\n",
            "1\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "3\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "1\n",
            "1024\n",
            "1024\n",
            "1024\n",
            "1024\n",
            "1024\n",
            "1\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "3\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "1\n",
            "1024\n",
            "1024\n",
            "1024\n",
            "1024\n",
            "1024\n",
            "1\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "3\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "256\n",
            "1\n",
            "1024\n",
            "1024\n",
            "1024\n",
            "1024\n",
            "1024\n",
            "1\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "3\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "1\n",
            "2048\n",
            "1\n",
            "2048\n",
            "2048\n",
            "2048\n",
            "2048\n",
            "2048\n",
            "2048\n",
            "2048\n",
            "2048\n",
            "2048\n",
            "1\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "3\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "1\n",
            "2048\n",
            "2048\n",
            "2048\n",
            "2048\n",
            "2048\n",
            "1\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "3\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "512\n",
            "1\n",
            "2048\n",
            "2048\n",
            "2048\n",
            "2048\n",
            "2048\n",
            "2048\n",
            "1024\n",
            "1024\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiL9bsaflSWY",
        "outputId": "a8059f89-c7e0-48eb-a50f-4240606dca24"
      },
      "source": [
        "model.layers[0].get_config()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_input_shape': (None, 32, 32, 3),\n",
              " 'dtype': 'float32',\n",
              " 'name': 'input_1',\n",
              " 'ragged': False,\n",
              " 'sparse': False}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4Sk-U_jKoTm"
      },
      "source": [
        "comms_round = 40\n",
        "loss='categorical_crossentropy'\n",
        "metrics = ['accuracy']\n",
        "optimizer = SGD(lr=0.01, \n",
        "                momentum=0.9\n",
        "               )   \n",
        "optimizer2 = SGD(lr=0.001,\n",
        "                 momentum=0.9\n",
        "               )   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rxt1ZYW77GRn"
      },
      "source": [
        "# model = MLP()\n",
        "# model = model.build()\n",
        "# model.compile(loss=loss, \n",
        "#                       optimizer=optimizer2, \n",
        "#                       metrics=metrics)\n",
        "# print((model.optimizer.lr))\n",
        "# model2 = MLP()\n",
        "# model2 = model2.build()\n",
        "# model2.compile(loss=loss, \n",
        "#                       optimizer=optimizer, \n",
        "#                       metrics=metrics)\n",
        "# print((model2.optimizer.lr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOd6IRXnKsL8"
      },
      "source": [
        "weight_scale = []\n",
        "for (name, data) in client_data.items():\n",
        "  weight_scale.append(len(data)/40000)\n",
        "weight_scale = np.array(weight_scale).reshape((len(weight_scale),1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCUMQ1SkLY_B"
      },
      "source": [
        "# def scale_model_weights(weight, scalar):\n",
        "#     weight_final = []\n",
        "#     steps = len(weight)\n",
        "#     for i in range(steps):\n",
        "#         weight_final.append(scalar * weight[i])\n",
        "#     return weight_final\n",
        "\n",
        "def sum_scaled_weights(scaled_weight_list, M, unlab_data):\n",
        "    #print(type(scaled_weight_list[0][0]))\n",
        "    avg_grad_mean = list()\n",
        "    avg_grad_var = list()\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        #print(np.shape(grad_list_tuple))\n",
        "        if len(np.shape(grad_list_tuple)) == 5:\n",
        "          w = weight_scale.reshape((len(weight_scale), 1, 1, 1, 1))\n",
        "        elif len(np.shape(grad_list_tuple)) == 4:\n",
        "          w = weight_scale.reshape((len(weight_scale), 1, 1, 1))\n",
        "        elif len(np.shape(grad_list_tuple)) == 3:\n",
        "          w = weight_scale.reshape((len(weight_scale), 1, 1))\n",
        "        else:\n",
        "          w = weight_scale\n",
        "        layer_mean = np.sum(grad_list_tuple*w, axis=0)\n",
        "        #print(np.shape(layer_mean))\n",
        "        layer_variance = np.sum(np.power(grad_list_tuple - layer_mean, 2)*w, axis = 0) \n",
        "        avg_grad_mean.append(layer_mean)\n",
        "        avg_grad_var.append(layer_variance)\n",
        "    return avg_grad_mean\n",
        "\n",
        "\n",
        "def test_model(X_test, Y_test,  model, comm_round):\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    logits = model.predict(X_test)\n",
        "    loss = cce(Y_test, logits)\n",
        "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
        "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
        "    return acc, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-qxqDSFM3LS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11263f5d-853f-4a78-c479-d3e313595084"
      },
      "source": [
        "mlp_global = MLP()\n",
        "global_model = mlp_global.build()\n",
        "\n",
        "for comm_round in range(comms_round):\n",
        "  \n",
        "    global_weights = global_model.get_weights()\n",
        "\n",
        "    scaled_local_weight_list = list()\n",
        "    client_names = list(client_batched.keys())\n",
        "    random.shuffle(client_names)\n",
        "    \n",
        "    for client in client_names:\n",
        "        def scheduler(epoch, lr):\n",
        "            if epoch  == 8:\n",
        "              lr = lr*0.1\n",
        "            if epoch == 15:\n",
        "              lr = lr*0.1  \n",
        "            return lr\n",
        "        mlp_local = MLP()\n",
        "        local_model = mlp_local.build()\n",
        "        local_model.compile(loss=loss, \n",
        "                      optimizer=SGD(lr=0.01,momentum=0.9), \n",
        "                      metrics=metrics)\n",
        "        local_model.set_weights(global_weights)\n",
        "        callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "        local_model.fit(client_batched[client], epochs=5, verbose=0, callbacks=[callback])\n",
        "        scaled_local_weight_list.append(local_model.get_weights())\n",
        "        K.clear_session()\n",
        "    avg = sum_scaled_weights(scaled_local_weight_list, 10, unlab_dataset)\n",
        "    mlp_global = MLP()\n",
        "    global_model = mlp_global.build()\n",
        "    global_model.compile(loss=loss, \n",
        "                      optimizer=SGD(lr=0.001,momentum=0.9), \n",
        "                      metrics=metrics)\n",
        "    global_model.set_weights(avg)\n",
        "\n",
        "    for(X_test, Y_test) in test_batched:\n",
        "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "comm_round: 0 | global_acc: 10.000% | global_loss: 2.3026137351989746\n",
            "comm_round: 1 | global_acc: 9.990% | global_loss: 2.299663782119751\n",
            "comm_round: 2 | global_acc: 19.830% | global_loss: 2.281569004058838\n",
            "comm_round: 3 | global_acc: 34.970% | global_loss: 2.2454729080200195\n",
            "comm_round: 4 | global_acc: 42.280% | global_loss: 2.2084739208221436\n",
            "comm_round: 5 | global_acc: 45.920% | global_loss: 2.1750223636627197\n",
            "comm_round: 6 | global_acc: 52.780% | global_loss: 2.1292898654937744\n",
            "comm_round: 7 | global_acc: 53.880% | global_loss: 2.108097791671753\n",
            "comm_round: 8 | global_acc: 55.870% | global_loss: 2.093919277191162\n",
            "comm_round: 9 | global_acc: 57.450% | global_loss: 2.0696821212768555\n",
            "comm_round: 10 | global_acc: 58.470% | global_loss: 2.0406908988952637\n",
            "comm_round: 11 | global_acc: 58.630% | global_loss: 2.036484718322754\n",
            "comm_round: 12 | global_acc: 59.520% | global_loss: 2.0228590965270996\n",
            "comm_round: 13 | global_acc: 61.390% | global_loss: 1.9987132549285889\n",
            "comm_round: 14 | global_acc: 62.040% | global_loss: 1.9896470308303833\n",
            "comm_round: 15 | global_acc: 63.230% | global_loss: 1.981309413909912\n",
            "comm_round: 16 | global_acc: 57.550% | global_loss: 2.010448455810547\n",
            "comm_round: 17 | global_acc: 62.690% | global_loss: 1.9735015630722046\n",
            "comm_round: 18 | global_acc: 62.660% | global_loss: 1.9571995735168457\n",
            "comm_round: 19 | global_acc: 63.810% | global_loss: 1.9588054418563843\n",
            "comm_round: 20 | global_acc: 62.360% | global_loss: 1.9493615627288818\n",
            "comm_round: 21 | global_acc: 64.300% | global_loss: 1.940858244895935\n",
            "comm_round: 22 | global_acc: 65.020% | global_loss: 1.93770432472229\n",
            "comm_round: 23 | global_acc: 64.030% | global_loss: 1.939725399017334\n",
            "comm_round: 24 | global_acc: 64.750% | global_loss: 1.9201974868774414\n",
            "comm_round: 25 | global_acc: 65.280% | global_loss: 1.9079035520553589\n",
            "comm_round: 26 | global_acc: 63.530% | global_loss: 1.917330265045166\n",
            "comm_round: 27 | global_acc: 62.270% | global_loss: 1.9300601482391357\n",
            "comm_round: 28 | global_acc: 61.200% | global_loss: 1.9377162456512451\n",
            "comm_round: 29 | global_acc: 61.610% | global_loss: 1.9203734397888184\n",
            "comm_round: 30 | global_acc: 61.330% | global_loss: 1.9125765562057495\n",
            "comm_round: 31 | global_acc: 62.910% | global_loss: 1.9122273921966553\n",
            "comm_round: 32 | global_acc: 61.980% | global_loss: 1.915755033493042\n",
            "comm_round: 33 | global_acc: 61.660% | global_loss: 1.9101488590240479\n",
            "comm_round: 34 | global_acc: 60.130% | global_loss: 1.9257285594940186\n",
            "comm_round: 35 | global_acc: 60.790% | global_loss: 1.9054234027862549\n",
            "comm_round: 36 | global_acc: 60.410% | global_loss: 1.9155913591384888\n",
            "comm_round: 37 | global_acc: 61.430% | global_loss: 1.9013879299163818\n",
            "comm_round: 38 | global_acc: 59.300% | global_loss: 1.9194257259368896\n",
            "comm_round: 39 | global_acc: 61.000% | global_loss: 1.904245138168335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjqygc_LTCqw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}