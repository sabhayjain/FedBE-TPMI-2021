{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yy46_-wlEr5g",
    "outputId": "5e54e288-a591-4d35-b90d-d6c149cd70cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-swa==0.1.5 in /usr/local/lib/python3.7/dist-packages (0.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-swa==0.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S16nLNsosyNk",
    "outputId": "06bccd9e-6340-4f31-d566-7b79b25eb6f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 11 14:04:00 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   71C    P0    33W /  70W |   1088MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wOBNu8TEu1A"
   },
   "outputs": [],
   "source": [
    "from swa.tfkeras import SWA\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stJrFfne1AtY"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8KlZ2ox1GwS"
   },
   "outputs": [],
   "source": [
    "(trainX, trainy), (testX, testy) = cifar10.load_data()\n",
    "trainX = trainX/255\n",
    "testX = testX/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hI7kn4F14Uv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0XVVFJt2P-u"
   },
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()                                  #Binarize labels in a one-vs-all fashion.\n",
    "trainy = lb.fit_transform(trainy)                     #Fit label binarizer and transform multi-class labels to binary labels\n",
    "testy = lb.fit_transform(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "825_bYug4h01"
   },
   "outputs": [],
   "source": [
    "def label_dict(img, labels):  # creating dictionary of labels\n",
    "  lab_lis = {}\n",
    "  for i in range(10):\n",
    "    lab_lis[i] = []          #declaring key values as labels  in dictionary\n",
    "  data = list(zip(img, labels))  # list of tuples of each image and label pair\n",
    "  for im, lab in data:\n",
    "    idx = np.argmax(lab)\n",
    "    lab_lis[idx].append((im, lab))\n",
    "  return lab_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_J40L6C4jdf"
   },
   "outputs": [],
   "source": [
    "t = label_dict(trainX, trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axv59HWr5w0e"
   },
   "outputs": [],
   "source": [
    "unlab_data = []\n",
    "unlab_img_data = []\n",
    "unlab_img_lab = []\n",
    "for i in range(len(t)):\n",
    "  unlab_data.extend(t[i][4000:]) # extracting 1000 imgaes from each class in the training set to generate the pool of so called unlabeled set\n",
    "for x,y in unlab_data:\n",
    "  unlab_img_data.append(x)\n",
    "  unlab_img_lab.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EIbri0xe79a2"
   },
   "outputs": [],
   "source": [
    "client_names = [\"client_\"+str(i) for i in range(10)]  # makiing non_iid datas set for each client\n",
    "client_data = {}\n",
    "for client in client_names:\n",
    "  client_data[client] = []\n",
    "i = 0\n",
    "for client in client_names:\n",
    "  client_data[client].extend(t[i%10][:1960])\n",
    "  client_data[client].extend(t[(i+1)%10][1960:3920])\n",
    "  i += 1\n",
    "for i in range(10):\n",
    "  k = 0\n",
    "  for j in range(10):\n",
    "    if j != (i)%10 and j != (i+1)%10:\n",
    "      client_data[\"client_\"+str(j)].extend(t[i][3920 + 10*k: 3920 + 10*(k+1)])\n",
    "      k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQitzXbYCdZj"
   },
   "outputs": [],
   "source": [
    "def batch(zip_data, batch_size = 40):\n",
    "  data, label = zip(*zip_data)  # Unzipping the img and label pair \n",
    "  dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label))) # create a dataset object to make use of other transformation attributes\n",
    "  return dataset.shuffle(len(label)).batch(batch_size)# shuffles data and creates batches of size 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwMs6hlaC7xH"
   },
   "outputs": [],
   "source": [
    "client_batched = {}\n",
    "for (client_name, data) in client_data.items():\n",
    "  client_batched[client_name] = batch(data)\n",
    "\n",
    "test_batched = tf.data.Dataset.from_tensor_slices((testX, testy)).batch(len(testy))\n",
    "unlab_dataset = tf.data.Dataset.from_tensor_slices((unlab_img_data)).batch(len(unlab_img_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5Lz3GrdGk6H"
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    @staticmethod\n",
    "    def build():\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3))) # 32 filters of size 3*3\n",
    "        model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(AveragePooling2D((2, 2))) # aveeraging the cell cvales for size of 2*2, witha stride of 2\n",
    "        model.add(Dropout(0.2))  # randomly assign the value o to nodes, with probability of 0.2\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(AveragePooling2D((2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(AveragePooling2D((2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2eVLxQaKGEV"
   },
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "model = model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_BmrbqbTKO37",
    "outputId": "6d1d368d-d907-47c4-e557-a3cc0d70f0d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_3 (Average (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_4 (Average (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "average_pooling2d_5 (Average (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 550,570\n",
      "Trainable params: 550,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2BzZ4rWaLpN4",
    "outputId": "edd9f430-b7fe-48d2-b3eb-71ab804903ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 3, 32)\n",
      "(32,)\n",
      "(3, 3, 32, 32)\n",
      "(32,)\n",
      "(3, 3, 32, 64)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(2048, 128)\n",
      "(128,)\n",
      "(128, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "t = model.get_weights()\n",
    "for v in t:\n",
    "  print(np.shape(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b4Sk-U_jKoTm"
   },
   "outputs": [],
   "source": [
    "comms_round = 45\n",
    "loss='categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = SGD(lr=0.01, \n",
    "                momentum=0.9\n",
    "               )   \n",
    "optimizer2 = SGD(lr=0.001,\n",
    "                 momentum=0.9\n",
    "               )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rxt1ZYW77GRn"
   },
   "outputs": [],
   "source": [
    "# model = MLP()\n",
    "# model = model.build()\n",
    "# model.compile(loss=loss, \n",
    "#                       optimizer=optimizer2, \n",
    "#                       metrics=metrics)\n",
    "# print((model.optimizer.lr))\n",
    "# model2 = MLP()\n",
    "# model2 = model2.build()\n",
    "# model2.compile(loss=loss, \n",
    "#                       optimizer=optimizer, \n",
    "#                       metrics=metrics)\n",
    "# print((model2.optimizer.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOd6IRXnKsL8"
   },
   "outputs": [],
   "source": [
    "weight_scale = []\n",
    "for (name, data) in client_data.items():\n",
    "  weight_scale.append(len(data)/40000)\n",
    "weight_scale = np.array(weight_scale).reshape((len(weight_scale),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCUMQ1SkLY_B"
   },
   "outputs": [],
   "source": [
    "# def scale_model_weights(weight, scalar):\n",
    "#     weight_final = []\n",
    "#     steps = len(weight)\n",
    "#     for i in range(steps):\n",
    "#         weight_final.append(scalar * weight[i])\n",
    "#     return weight_final\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list, M, unlab_data):\n",
    "    #print(type(scaled_weight_list[0][0]))\n",
    "    avg_grad_mean = list()\n",
    "    avg_grad_var = list()\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        #print(np.shape(grad_list_tuple))\n",
    "        if len(np.shape(grad_list_tuple)) == 5:\n",
    "          w = weight_scale.reshape((len(weight_scale), 1, 1, 1, 1))\n",
    "        elif len(np.shape(grad_list_tuple)) == 4:\n",
    "          w = weight_scale.reshape((len(weight_scale), 1, 1, 1))\n",
    "        elif len(np.shape(grad_list_tuple)) == 3:\n",
    "          w = weight_scale.reshape((len(weight_scale), 1, 1))\n",
    "        else:\n",
    "          w = weight_scale\n",
    "        layer_mean = np.sum(grad_list_tuple*w, axis=0)\n",
    "        #print(np.shape(layer_mean))\n",
    "        layer_variance = np.sum(np.power(grad_list_tuple - layer_mean, 2)*w, axis = 0) \n",
    "        avg_grad_mean.append(layer_mean)\n",
    "        avg_grad_var.append(layer_variance)\n",
    "    return avg_grad_mean\n",
    "\n",
    "\n",
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    logits = model.predict(X_test)\n",
    "    loss = cce(Y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
    "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-qxqDSFM3LS",
    "outputId": "2ffc99e0-8de9-450b-f57c-efee41e9e5f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comm_round: 0 | global_acc: 13.630% | global_loss: 2.300297975540161\n",
      "comm_round: 1 | global_acc: 26.850% | global_loss: 2.274691581726074\n",
      "comm_round: 2 | global_acc: 42.240% | global_loss: 2.1918423175811768\n",
      "comm_round: 3 | global_acc: 48.610% | global_loss: 2.109875440597534\n",
      "comm_round: 4 | global_acc: 53.950% | global_loss: 2.0326969623565674\n",
      "comm_round: 5 | global_acc: 57.250% | global_loss: 1.9741109609603882\n",
      "comm_round: 6 | global_acc: 59.710% | global_loss: 1.9329475164413452\n",
      "comm_round: 7 | global_acc: 61.590% | global_loss: 1.902298092842102\n",
      "comm_round: 8 | global_acc: 62.970% | global_loss: 1.8830710649490356\n",
      "comm_round: 9 | global_acc: 63.640% | global_loss: 1.8627345561981201\n",
      "comm_round: 10 | global_acc: 64.710% | global_loss: 1.8530644178390503\n",
      "comm_round: 11 | global_acc: 65.050% | global_loss: 1.8445842266082764\n",
      "comm_round: 12 | global_acc: 65.790% | global_loss: 1.8380894660949707\n",
      "comm_round: 13 | global_acc: 66.110% | global_loss: 1.8324648141860962\n",
      "comm_round: 14 | global_acc: 66.690% | global_loss: 1.8240443468093872\n",
      "comm_round: 15 | global_acc: 67.090% | global_loss: 1.8205713033676147\n",
      "comm_round: 16 | global_acc: 66.820% | global_loss: 1.8188008069992065\n",
      "comm_round: 17 | global_acc: 67.400% | global_loss: 1.8143525123596191\n",
      "comm_round: 18 | global_acc: 67.540% | global_loss: 1.8105300664901733\n",
      "comm_round: 19 | global_acc: 67.790% | global_loss: 1.8085757493972778\n",
      "comm_round: 20 | global_acc: 67.700% | global_loss: 1.8066937923431396\n",
      "comm_round: 21 | global_acc: 67.680% | global_loss: 1.8033568859100342\n",
      "comm_round: 22 | global_acc: 68.040% | global_loss: 1.8023226261138916\n",
      "comm_round: 23 | global_acc: 68.690% | global_loss: 1.7995084524154663\n",
      "comm_round: 24 | global_acc: 68.390% | global_loss: 1.7988042831420898\n",
      "comm_round: 25 | global_acc: 68.480% | global_loss: 1.7979425191879272\n",
      "comm_round: 26 | global_acc: 68.330% | global_loss: 1.7966947555541992\n",
      "comm_round: 27 | global_acc: 68.720% | global_loss: 1.7943679094314575\n",
      "comm_round: 28 | global_acc: 68.500% | global_loss: 1.7936880588531494\n",
      "comm_round: 29 | global_acc: 69.040% | global_loss: 1.7921816110610962\n",
      "comm_round: 30 | global_acc: 68.540% | global_loss: 1.7926661968231201\n",
      "comm_round: 31 | global_acc: 68.790% | global_loss: 1.791627287864685\n",
      "comm_round: 32 | global_acc: 68.970% | global_loss: 1.789833426475525\n",
      "comm_round: 33 | global_acc: 68.860% | global_loss: 1.7908613681793213\n",
      "comm_round: 34 | global_acc: 69.490% | global_loss: 1.7863937616348267\n",
      "comm_round: 35 | global_acc: 69.220% | global_loss: 1.7857177257537842\n",
      "comm_round: 36 | global_acc: 69.390% | global_loss: 1.7849900722503662\n",
      "comm_round: 37 | global_acc: 69.600% | global_loss: 1.784264087677002\n",
      "comm_round: 38 | global_acc: 69.400% | global_loss: 1.782591462135315\n",
      "comm_round: 39 | global_acc: 69.790% | global_loss: 1.7827237844467163\n",
      "comm_round: 40 | global_acc: 69.710% | global_loss: 1.7823445796966553\n",
      "comm_round: 41 | global_acc: 69.550% | global_loss: 1.7802034616470337\n",
      "comm_round: 42 | global_acc: 69.980% | global_loss: 1.7795077562332153\n",
      "comm_round: 43 | global_acc: 69.660% | global_loss: 1.7799359560012817\n",
      "comm_round: 44 | global_acc: 69.820% | global_loss: 1.7804423570632935\n"
     ]
    }
   ],
   "source": [
    "mlp_global = MLP()\n",
    "global_model = mlp_global.build()\n",
    "\n",
    "for comm_round in range(comms_round):\n",
    "            \n",
    "    \n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    \n",
    "    scaled_local_weight_list = list()\n",
    "    client_names = list(client_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "    \n",
    "    for client in client_names:\n",
    "        def scheduler(epoch, lr):\n",
    "            if epoch  == 8:\n",
    "              lr = lr*0.1\n",
    "            if epoch == 15:\n",
    "              lr = lr*0.1  \n",
    "            return lr\n",
    "        mlp_local = MLP()\n",
    "        local_model = mlp_local.build()\n",
    "        local_model.compile(loss=loss, \n",
    "                      optimizer=SGD(lr=0.01,momentum=0.9), \n",
    "                      metrics=metrics)\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "        local_model.fit(client_batched[client], epochs=25, verbose=0, callbacks=[callback])\n",
    "        scaled_local_weight_list.append(local_model.get_weights())\n",
    "        \n",
    "        K.clear_session()\n",
    "    avg = sum_scaled_weights(scaled_local_weight_list, 10, unlab_dataset)\n",
    "    mlp_global = MLP()\n",
    "    global_model = mlp_global.build()\n",
    "    global_model.compile(loss=loss, \n",
    "                      optimizer=SGD(lr=0.001,momentum=0.9), \n",
    "                      metrics=metrics)\n",
    "    global_model.set_weights(avg)\n",
    "\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdEgGGUHw6sw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FEDAVG_NonIID.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
