{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yy46_-wlEr5g",
    "outputId": "e92c14d5-fdb2-4c73-eb81-9edf75aad3af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-swa==0.1.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/40/2796068a5d40c9b7f8d2b0d5a830a45c3ff9909194931f50a87ec19f45d5/keras-swa-0.1.5.tar.gz (77kB)\n",
      "\r",
      "\u001b[K     |████▎                           | 10kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 20kB 17.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▊                   | 30kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 40kB 13.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 51kB 7.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 61kB 9.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 71kB 8.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 81kB 4.8MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: keras-swa\n",
      "  Building wheel for keras-swa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-swa: filename=keras_swa-0.1.5-cp37-none-any.whl size=9714 sha256=24c3c3a6298e9e2e59c848f0186576b191b593c1c30bac05084b111d6c2cb511\n",
      "  Stored in directory: /root/.cache/pip/wheels/30/de/a7/f2a71861bb678729a45e8995dc2496118e875e64b89379c530\n",
      "Successfully built keras-swa\n",
      "Installing collected packages: keras-swa\n",
      "Successfully installed keras-swa-0.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-swa==0.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wOBNu8TEu1A"
   },
   "outputs": [],
   "source": [
    "from swa.tfkeras import SWA\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stJrFfne1AtY"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s8KlZ2ox1GwS",
    "outputId": "dd586a0c-32d9-41e7-9888-5ca77ad1e756"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(trainX, trainy), (testX, testy) = cifar10.load_data()\n",
    "trainX = trainX/255\n",
    "testX = testX/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hI7kn4F14Uv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0XVVFJt2P-u"
   },
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "trainy = lb.fit_transform(trainy)\n",
    "testy = lb.fit_transform(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "825_bYug4h01"
   },
   "outputs": [],
   "source": [
    "def label_dict(img, labels):\n",
    "  lab_lis = {}\n",
    "  for i in range(10):\n",
    "    lab_lis[i] = []\n",
    "  data = list(zip(img, labels))\n",
    "  for im, lab in data:\n",
    "    idx = np.argmax(lab)\n",
    "    lab_lis[idx].append((im, lab))\n",
    "  return lab_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_J40L6C4jdf"
   },
   "outputs": [],
   "source": [
    "t = label_dict(trainX, trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axv59HWr5w0e"
   },
   "outputs": [],
   "source": [
    "unlab_data = []\n",
    "unlab_img_data = []\n",
    "for i in range(len(t)):\n",
    "  unlab_data.extend(t[i][4000:])\n",
    "for x,y in unlab_data:\n",
    "  unlab_img_data.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "79bkiQUpE5ET",
    "outputId": "6348cb76-b734-473c-af50-18b2294bf5ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.99607843, 0.99607843, 0.99607843],\n",
       "        [0.98823529, 0.98823529, 0.98823529],\n",
       "        [0.98823529, 0.98823529, 0.98431373],\n",
       "        ...,\n",
       "        [0.98823529, 0.98823529, 0.99215686],\n",
       "        [0.98823529, 0.98823529, 0.98039216],\n",
       "        [1.        , 1.        , 0.99607843]],\n",
       "\n",
       "       [[1.        , 1.        , 1.        ],\n",
       "        [0.99607843, 0.99607843, 0.99607843],\n",
       "        [0.99607843, 0.99607843, 0.98823529],\n",
       "        ...,\n",
       "        [0.99607843, 0.99607843, 1.        ],\n",
       "        [0.99607843, 0.99607843, 0.98823529],\n",
       "        [1.        , 1.        , 1.        ]],\n",
       "\n",
       "       [[1.        , 1.        , 1.        ],\n",
       "        [0.99607843, 0.99607843, 0.99607843],\n",
       "        [1.        , 0.99607843, 0.99215686],\n",
       "        ...,\n",
       "        [0.99607843, 0.99607843, 1.        ],\n",
       "        [0.99607843, 0.99607843, 0.98823529],\n",
       "        [1.        , 1.        , 1.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.96862745, 0.94509804, 0.95294118],\n",
       "        [0.96078431, 0.93333333, 0.94117647],\n",
       "        [0.96470588, 0.9372549 , 0.9372549 ],\n",
       "        ...,\n",
       "        [0.96470588, 0.9372549 , 0.94509804],\n",
       "        [0.96470588, 0.9372549 , 0.94509804],\n",
       "        [0.96862745, 0.94901961, 0.95686275]],\n",
       "\n",
       "       [[0.68235294, 0.69803922, 0.67843137],\n",
       "        [0.69019608, 0.68235294, 0.67058824],\n",
       "        [0.70196078, 0.67843137, 0.67058824],\n",
       "        ...,\n",
       "        [0.68235294, 0.69019608, 0.69411765],\n",
       "        [0.6745098 , 0.69803922, 0.69019608],\n",
       "        [0.68627451, 0.70196078, 0.70196078]],\n",
       "\n",
       "       [[0.67843137, 0.71764706, 0.69019608],\n",
       "        [0.68235294, 0.69411765, 0.6745098 ],\n",
       "        [0.69411765, 0.68627451, 0.6745098 ],\n",
       "        ...,\n",
       "        [0.70588235, 0.83529412, 0.82352941],\n",
       "        [0.69411765, 0.83921569, 0.82352941],\n",
       "        [0.70196078, 0.84705882, 0.83921569]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlab_img_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EIbri0xe79a2"
   },
   "outputs": [],
   "source": [
    "client_names = [\"client_\"+str(i) for i in range(10)]\n",
    "client_data = {}\n",
    "for client in client_names:\n",
    "  client_data[client] = []\n",
    "i = 0\n",
    "for client in client_names:\n",
    "  client_data[client].extend(t[i%10][:1960])\n",
    "  client_data[client].extend(t[(i+1)%10][1960:3920])\n",
    "  i += 1\n",
    "for i in range(10):\n",
    "  k = 0\n",
    "  for j in range(10):\n",
    "    if j != (i)%10 and j != (i+1)%10:\n",
    "      client_data[\"client_\"+str(j)].extend(t[i][3920 + 10*k: 3920 + 10*(k+1)])\n",
    "      k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQitzXbYCdZj"
   },
   "outputs": [],
   "source": [
    "def batch(zip_data, batch_size = 40):\n",
    "  data, label = zip(*zip_data)\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "  return dataset.shuffle(len(label)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwMs6hlaC7xH"
   },
   "outputs": [],
   "source": [
    "client_batched = {}\n",
    "for (name, data) in client_data.items():\n",
    "  client_batched[name] = batch(data)\n",
    "\n",
    "test_batched = tf.data.Dataset.from_tensor_slices((testX, testy)).batch(len(testy))\n",
    "unlab_dataset = tf.data.Dataset.from_tensor_slices((unlab_img_data)).batch(len(unlab_img_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5Lz3GrdGk6H"
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    @staticmethod\n",
    "    def build():\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
    "        model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(AveragePooling2D((2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(AveragePooling2D((2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(AveragePooling2D((2, 2)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2eVLxQaKGEV"
   },
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "model = model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_BmrbqbTKO37",
    "outputId": "95303ac9-7860-4886-f82b-ed6f9b7b68bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 550,570\n",
      "Trainable params: 550,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2BzZ4rWaLpN4",
    "outputId": "2285dcc8-ac15-40a9-fe66-cca666a1adeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 3, 32)\n",
      "(32,)\n",
      "(3, 3, 32, 32)\n",
      "(32,)\n",
      "(3, 3, 32, 64)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(2048, 128)\n",
      "(128,)\n",
      "(128, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "t = model.get_weights()\n",
    "for v in t:\n",
    "  print(np.shape(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b4Sk-U_jKoTm"
   },
   "outputs": [],
   "source": [
    "comms_round = 50\n",
    "loss='categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = SGD(lr=0.01, \n",
    "                momentum=0.9\n",
    "               )   \n",
    "optimizer2 = SGD(lr=0.001,\n",
    "                 momentum=0.9\n",
    "               )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WYbxOUreCDai",
    "outputId": "7bc43edc-69a6-4681-d0c9-ffdcfd75b688"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 11 18:33:21 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   67C    P0    31W /  70W |    230MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rxt1ZYW77GRn"
   },
   "outputs": [],
   "source": [
    "# model = MLP()\n",
    "# model = model.build()\n",
    "# model.compile(loss=loss, \n",
    "#                       optimizer=optimizer2, \n",
    "#                       metrics=metrics)\n",
    "# print((model.optimizer.lr))\n",
    "# model2 = MLP()\n",
    "# model2 = model2.build()\n",
    "# model2.compile(loss=loss, \n",
    "#                       optimizer=optimizer, \n",
    "#                       metrics=metrics)\n",
    "# print((model2.optimizer.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOd6IRXnKsL8"
   },
   "outputs": [],
   "source": [
    "weight_scale = []\n",
    "for (name, data) in client_data.items():\n",
    "  weight_scale.append(len(data)/40000)\n",
    "weight_scale = np.array(weight_scale).reshape((len(weight_scale), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCUMQ1SkLY_B"
   },
   "outputs": [],
   "source": [
    "# def scale_model_weights(weight, scalar):\n",
    "#     weight_final = []\n",
    "#     steps = len(weight)\n",
    "#     for i in range(steps):\n",
    "#         weight_final.append(scalar * weight[i])\n",
    "#     return weight_final\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list, M, unlab_data):\n",
    "    #print(type(scaled_weight_list[0][0]))\n",
    "    temp = MLP()\n",
    "    model = temp.build()\n",
    "    p = np.zeros((10000,10))\n",
    "    for i in range(len(weight_scale)):\n",
    "      model.set_weights(scaled_weight_list[i])\n",
    "      p += model.predict(unlab_data)\n",
    "    avg_grad_mean = list()\n",
    "    avg_grad_var = list()\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        #print(np.shape(grad_list_tuple))\n",
    "        if len(np.shape(grad_list_tuple)) == 5:\n",
    "          w = weight_scale.reshape((len(weight_scale), 1, 1, 1, 1))\n",
    "        elif len(np.shape(grad_list_tuple)) == 4:\n",
    "          w = weight_scale.reshape((len(weight_scale), 1, 1, 1))\n",
    "        elif len(np.shape(grad_list_tuple)) == 3:\n",
    "          w = weight_scale.reshape((len(weight_scale), 1, 1))\n",
    "        else:\n",
    "          w = weight_scale\n",
    "        layer_mean = np.sum(grad_list_tuple*w, axis=0)\n",
    "        #print(np.shape(layer_mean))\n",
    "        layer_variance = np.sum(np.power(grad_list_tuple - layer_mean, 2)*w, axis = 0) \n",
    "        avg_grad_mean.append(layer_mean)\n",
    "        avg_grad_var.append(layer_variance)\n",
    "    model.set_weights(avg_grad_mean)\n",
    "    p += model.predict(unlab_data)\n",
    "    for i in range(M):\n",
    "      grad_list = list()\n",
    "      for j in range(len(avg_grad_var)):\n",
    "        grad_list.append(np.random.normal(avg_grad_mean[j], avg_grad_var[j]))\n",
    "      model.set_weights(grad_list)\n",
    "      p += model.predict(unlab_data)\n",
    "    return p/(M + len(weight_scale) + 1), avg_grad_mean\n",
    "\n",
    "\n",
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    logits = model.predict(X_test)\n",
    "    loss = cce(Y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
    "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-qxqDSFM3LS",
    "outputId": "4a7fbb3a-5e1e-4531-8377-3d71f999f8c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comm_round: 0 | global_acc: 16.640% | global_loss: 2.28967547416687\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0082s vs `on_train_batch_end` time: 0.0111s). Check your callbacks.\n",
      "comm_round: 1 | global_acc: 34.950% | global_loss: 2.213949203491211\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0087s vs `on_train_batch_end` time: 0.0109s). Check your callbacks.\n",
      "comm_round: 2 | global_acc: 44.900% | global_loss: 2.1034395694732666\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0099s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 3 | global_acc: 51.120% | global_loss: 2.027338743209839\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0084s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 4 | global_acc: 54.360% | global_loss: 1.9775562286376953\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0086s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 5 | global_acc: 57.390% | global_loss: 1.939963698387146\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0084s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 6 | global_acc: 59.610% | global_loss: 1.91257905960083\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0084s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 7 | global_acc: 61.170% | global_loss: 1.8934775590896606\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0084s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 8 | global_acc: 62.420% | global_loss: 1.8772881031036377\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0084s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 9 | global_acc: 63.480% | global_loss: 1.866194486618042\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0085s vs `on_train_batch_end` time: 0.0111s). Check your callbacks.\n",
      "comm_round: 10 | global_acc: 64.620% | global_loss: 1.8543281555175781\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0083s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 11 | global_acc: 65.300% | global_loss: 1.8467133045196533\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0083s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 12 | global_acc: 66.110% | global_loss: 1.8391326665878296\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0084s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 13 | global_acc: 66.540% | global_loss: 1.8312523365020752\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0085s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 14 | global_acc: 66.980% | global_loss: 1.8256646394729614\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0085s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 15 | global_acc: 67.890% | global_loss: 1.8195788860321045\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0085s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 16 | global_acc: 68.000% | global_loss: 1.8147456645965576\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0083s vs `on_train_batch_end` time: 0.0111s). Check your callbacks.\n",
      "comm_round: 17 | global_acc: 68.300% | global_loss: 1.811801791191101\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0083s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 18 | global_acc: 68.780% | global_loss: 1.8073776960372925\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0083s vs `on_train_batch_end` time: 0.0111s). Check your callbacks.\n",
      "comm_round: 19 | global_acc: 68.890% | global_loss: 1.8055055141448975\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0085s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 20 | global_acc: 69.330% | global_loss: 1.803114414215088\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0084s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 21 | global_acc: 69.230% | global_loss: 1.8009629249572754\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0095s vs `on_train_batch_end` time: 0.0109s). Check your callbacks.\n",
      "comm_round: 22 | global_acc: 69.610% | global_loss: 1.7978097200393677\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0086s vs `on_train_batch_end` time: 0.0109s). Check your callbacks.\n",
      "comm_round: 23 | global_acc: 69.970% | global_loss: 1.7957669496536255\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0085s vs `on_train_batch_end` time: 0.0111s). Check your callbacks.\n",
      "comm_round: 24 | global_acc: 69.960% | global_loss: 1.7926909923553467\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0083s vs `on_train_batch_end` time: 0.0111s). Check your callbacks.\n",
      "comm_round: 25 | global_acc: 70.430% | global_loss: 1.790493130683899\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0084s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 26 | global_acc: 70.080% | global_loss: 1.7905105352401733\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0088s vs `on_train_batch_end` time: 0.0108s). Check your callbacks.\n",
      "comm_round: 27 | global_acc: 70.290% | global_loss: 1.7879537343978882\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0085s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 28 | global_acc: 70.660% | global_loss: 1.7866969108581543\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0085s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 29 | global_acc: 70.460% | global_loss: 1.786254644393921\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0084s vs `on_train_batch_end` time: 0.0113s). Check your callbacks.\n",
      "comm_round: 30 | global_acc: 70.620% | global_loss: 1.7854888439178467\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0086s vs `on_train_batch_end` time: 0.0109s). Check your callbacks.\n",
      "comm_round: 31 | global_acc: 70.790% | global_loss: 1.7839581966400146\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0084s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 32 | global_acc: 70.850% | global_loss: 1.7828102111816406\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0083s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 33 | global_acc: 71.010% | global_loss: 1.7815593481063843\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0082s vs `on_train_batch_end` time: 0.0111s). Check your callbacks.\n",
      "comm_round: 34 | global_acc: 70.990% | global_loss: 1.7811616659164429\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0083s vs `on_train_batch_end` time: 0.0112s). Check your callbacks.\n",
      "comm_round: 35 | global_acc: 71.200% | global_loss: 1.780112862586975\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0085s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 36 | global_acc: 71.390% | global_loss: 1.7789835929870605\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0086s vs `on_train_batch_end` time: 0.0114s). Check your callbacks.\n",
      "comm_round: 37 | global_acc: 71.420% | global_loss: 1.7785910367965698\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0084s vs `on_train_batch_end` time: 0.0111s). Check your callbacks.\n",
      "comm_round: 38 | global_acc: 71.510% | global_loss: 1.777613639831543\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0084s vs `on_train_batch_end` time: 0.0115s). Check your callbacks.\n",
      "comm_round: 39 | global_acc: 71.430% | global_loss: 1.7770620584487915\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0101s vs `on_train_batch_end` time: 0.0109s). Check your callbacks.\n",
      "comm_round: 40 | global_acc: 71.510% | global_loss: 1.7764519453048706\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0085s vs `on_train_batch_end` time: 0.0115s). Check your callbacks.\n",
      "comm_round: 41 | global_acc: 71.520% | global_loss: 1.776277780532837\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0086s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 42 | global_acc: 71.730% | global_loss: 1.7744214534759521\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0084s vs `on_train_batch_end` time: 0.0109s). Check your callbacks.\n",
      "comm_round: 43 | global_acc: 71.570% | global_loss: 1.7745741605758667\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0088s vs `on_train_batch_end` time: 0.0111s). Check your callbacks.\n",
      "comm_round: 44 | global_acc: 71.700% | global_loss: 1.77323317527771\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0084s vs `on_train_batch_end` time: 0.0111s). Check your callbacks.\n",
      "comm_round: 45 | global_acc: 71.880% | global_loss: 1.7726277112960815\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0084s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 46 | global_acc: 71.880% | global_loss: 1.7728534936904907\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0084s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "comm_round: 47 | global_acc: 71.930% | global_loss: 1.7713706493377686\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0083s vs `on_train_batch_end` time: 0.0111s). Check your callbacks.\n",
      "comm_round: 48 | global_acc: 72.020% | global_loss: 1.770367980003357\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0082s vs `on_train_batch_end` time: 0.0111s). Check your callbacks.\n",
      "comm_round: 49 | global_acc: 71.940% | global_loss: 1.771600604057312\n"
     ]
    }
   ],
   "source": [
    "mlp_global = MLP()\n",
    "global_model = mlp_global.build()\n",
    "\n",
    "for comm_round in range(comms_round):\n",
    "            \n",
    "    \n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    \n",
    "    scaled_local_weight_list = list()\n",
    "    client_names = list(client_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "    \n",
    "    for client in client_names:\n",
    "        def scheduler(epoch, lr):\n",
    "            if epoch  == 8:\n",
    "              lr = lr*0.1\n",
    "            if epoch == 15:\n",
    "              lr = lr*0.1  \n",
    "            return lr\n",
    "        mlp_local = MLP()\n",
    "        local_model = mlp_local.build()\n",
    "        local_model.compile(loss=loss, \n",
    "                      optimizer=SGD(lr=0.01,momentum=0.9), \n",
    "                      metrics=metrics)\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "        local_model.fit(client_batched[client], epochs=25, verbose=0, callbacks=[callback])\n",
    "        scaled_local_weight_list.append(local_model.get_weights())\n",
    "        \n",
    "        K.clear_session()\n",
    "    p, avg = sum_scaled_weights(scaled_local_weight_list, 10, unlab_dataset)\n",
    "    p = p**2/(np.sum(p**2, axis = 1).reshape((len(p), 1)))\n",
    "\n",
    "    lab_dataset = tf.data.Dataset.from_tensor_slices((unlab_img_data,p)).batch(128)\n",
    "    mlp_global = MLP()\n",
    "    global_model = mlp_global.build()\n",
    "    global_model.compile(loss=loss, \n",
    "                      optimizer=SGD(lr=0.001,momentum=0.9), \n",
    "                      metrics=metrics)\n",
    "    global_model.set_weights(avg)\n",
    "    swa = SWA(start_epoch=5, \n",
    "          lr_schedule='cyclic', \n",
    "          swa_lr=0.0004,\n",
    "          swa_lr2=0.001,\n",
    "          swa_freq=5,\n",
    "          verbose=0)\n",
    "    global_model.fit(lab_dataset, epochs = 20, verbose = 0, callbacks=[swa])\n",
    "\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdEgGGUHw6sw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FedBE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
